# -*- coding: utf-8 -*-
"""workingmomentum.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZMJGM-aLrcicGvojVQqspaxjsseCZY5Q
"""

import numpy as np
from keras.datasets import fashion_mnist
(train_X,train_Y),(test_X,test_Y)=fashion_mnist.load_data()
#normalize the train dataset as we values are from 0-255
train_X=train_X/255
test_X=test_X/255

#transforming dataset to 2d matrix whith col representing the data  
no_of_datapoints=train_X.shape[0]
col_dim=train_X.shape[1]*train_X.shape[2]
x_data=train_X.reshape(no_of_datapoints,col_dim)
x_data=x_data.T   #input is x_data

#trainform the test dataset 
no_of_datapoints_test=test_X.shape[0]
x_test=test_X.reshape(no_of_datapoints_test,col_dim)
x_test=x_test.T  #to test use this data

def init_layer(no_neuron_input,no_neuron_output,weight,bias):
 
  #weight.append(np.random.normal(0,0.5,(no_neuron_output,no_neuron_input))) #128*784
  #bias.append(np.random.normal(0,0.5,(no_neuron_output,1)))
  weight.append(np.random.uniform(-1,1,(no_neuron_output,no_neuron_input)))
  bias.append(np.random.normal(-1,1,(no_neuron_output,1)))

def sigmoid(z):
  g = 1 / (1 + np.exp(-z))
  return g

def softmax(x):
  x=x.T
  for i in range(x.shape[0]):
    sum=0
    for j in range(x.shape[1]):
      sum+=np.exp(x[i][j])
    x[i]=np.exp(x[i])/sum
  x=x.T

  return x

def sigmoid_der(z):
  return ((1/(1+np.exp(-z)))*(1-((1/(1+np.exp(-z))))))

def forward_propogation(input,weight,bias,no_of_hidden_layers,output):
  activation=[]  #input to above layer
  preactivation=[] 
  #no of layers =  no of layers
  #a=wx+b   and h is sigmoid of a   input data is 
  activation.append(input)
  preactivation.append(input)
  for i in range(no_of_hidden_layers):
    temp_preactive = np.matmul(weight[i],activation[i])+bias[i]
    preactivation.append(temp_preactive)
    activation.append(sigmoid(np.copy(temp_preactive)))
  
  temp_preactive=np.matmul(weight[-1],activation[-1])+bias[-1]
  preactivation.append(temp_preactive)
  activation.append(softmax(np.copy(temp_preactive)))

  loss=0
  temp=activation[-1].T
  for i in range(input.shape[1]):
    loss+=-np.log2(temp[i][output[i]])
  return activation,loss,preactivation

def backward_propogation(activation,output,no_hidden_layers,weights,a,batch_size):
  #gradient_activation=[]
  gradient_weight=[]
  gradient_bias=[]
  
  # we need one hot vector for output in the format 10*60000 
  hot=np.zeros((activation[-1].shape[1],activation[-1].shape[0]))
  for i in range(activation[-1].shape[1]):
    hot[i][output[i]]=1
  
  hot=hot.T
  gradient_weight=[]
  gradiend_bias=[]
  gradient_activation=-(hot-activation[-1])  #gradient al

  for i in range(no_hidden_layers,-1,-1):
    weight_temp=np.matmul(gradient_activation,activation[i].T)/batch_size
    bias_temp=(np.sum(gradient_activation,axis=1)/batch_size).reshape(-1,1)
    gradient_weight.append(weight_temp)
    gradient_bias.append(bias_temp)
    #print(gradient_bias[i].shape)
    if(i!=0):
      gradient_temp=np.matmul(weights[i].T,gradient_activation)
      #gradient_activation=gradient_temp*sigmoid_der(a[i]) thhis below is for sigmoid baki ka likhna hai relu tanh a ias preactivation
      gradient_activation=gradient_temp*sigmoid_der(a[i])
  gradient_weight.reverse()
  gradient_bias.reverse()
  return gradient_weight,gradient_bias

def backward_propogation_momentum(activation,output,no_hidden_layers,weights,a,batch_size):
  #gradient_activation=[]
  gradient_weight=[]
  gradient_bias=[]
  
  # we need one hot vector for output in the format 10*60000 
  hot=np.zeros((activation[-1].shape[1],activation[-1].shape[0]))
  for i in range(activation[-1].shape[1]):
    hot[i][output[i]]=1
  
  hot=hot.T
  gradient_weight=[]
  gradiend_bias=[]
  gradient_activation=-(hot-activation[-1])  #gradient al

  for i in range(no_hidden_layers,-1,-1):
    weight_temp=np.matmul(gradient_activation,activation[i].T)/batch_size
    bias_temp=(np.sum(gradient_activation,axis=1)/batch_size).reshape(-1,1)
    gradient_weight.append(weight_temp)
    gradient_bias.append(bias_temp)
    
    #print(gradient_bias[i].shape)
    if(i!=0):
      gradient_temp=np.matmul(weights[i].T,gradient_activation)
      #gradient_activation=gradient_temp*sigmoid_der(a[i]) thhis below is for sigmoid baki ka likhna hai relu tanh a ias preactivation
      gradient_activation=gradient_temp*sigmoid_der(np.copy(a[i]))
  gradient_weight.reverse()
  gradient_bias.reverse()
  return gradient_weight,gradient_bias

def gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size):
  for i in range(epochs):
    activation,loss,a=forward_propogation(input,weight,bias,no_of_hidden_layers,output)
    gradient_weight,gradient_bias=backward_propogation(activation,output,no_of_hidden_layers,weight,a,batch_size) 
    for i in range(len(weight)):
      weight[i]=weight[i]-learning_rate*gradient_weight[i]
      bias[i]=bias[i]-learning_rate*gradient_bias[i]

    print(loss/input.shape[1])

def batch_gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size):
  for i in range(epochs):
    # take theinput and spilt it in batch and then provide that as input to
    for j in range(0,input.shape[1],batch_size):
      x_batch= input[:,j:j+batch_size]
      y_batch=  output[j:j+batch_size] 
      activation,loss,a=forward_propogation(x_batch,weight,bias,no_of_hidden_layers,y_batch)
      gradient_weight,gradient_bias=backward_propogation(activation,y_batch,no_of_hidden_layers,weight,a,batch_size)
      
      for k in range(len(weight)):
        weight[k]=weight[k]-learning_rate*gradient_weight[k]
        bias[k]=bias[k]-learning_rate*gradient_bias[k]

    print("epoch is ", i ," j from " ,j,"to", j+batch_size,loss/batch_size)

def momentum_gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size,beta):
  ut_weight=[]
  ut_bias=[]
  ut_weight.extend([0 for i in range(len(weight))])
  ut_bias.extend([0 for i in range(len(bias))])
  
  for i in range(epochs):
    # take theinput and spilt it in batch and then provide that as input to
    for j in range(0,input.shape[1],batch_size):
      x_batch= input[:,j:j+batch_size]
      y_batch=  output[j:j+batch_size] 
      activation,loss,a=forward_propogation(x_batch,weight,bias,no_of_hidden_layers,y_batch)
      gradient_weight,gradient_bias=backward_propogation(activation,y_batch,no_of_hidden_layers,weight,a,batch_size)

      

      for k in range(len(weight)):
        ut_weight[k]=beta*ut_weight[k]+learning_rate*gradient_weight[k]
        ut_bias[k]=beta*ut_bias[k]+learning_rate*gradient_bias[k]
        weight[k]=weight[k]-ut_weight[k]
        bias[k]=bias[k]-ut_bias[k]

    print("epoch is ", i ," j from " ,j,"to", j+batch_size,loss/batch_size)

weight=[]
bias=[]

init_layer(784,128,weight,bias)
init_layer(128,128,weight,bias)
init_layer(128,128,weight,bias)
init_layer(128,10,weight,bias)

no_hidden_layer=len(bias)-1



gradient_descent(x_data,weight,bias,len(weight)-1,train_Y,15,0.001,x_data.shape[1])

batch_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,2,0.1,32)

momentum_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.1,32,0.9)

momentum_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.1,32,0.9)

