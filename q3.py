# -*- coding: utf-8 -*-
"""workinggraddesc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NslUt22qvBeEz6GUpT6_jqgtntUv9PkI
"""

import numpy as np
from keras.datasets import fashion_mnist
(train_X,train_Y),(test_X,test_Y)=fashion_mnist.load_data()
#normalize the train dataset as he valoes are from 0-255
train_X=train_X/255
test_X=test_X/255

#transforming dataset and then 
no_of_datapoints=train_X.shape[0]
col_dim=train_X.shape[1]*train_X.shape[2]
x_data=train_X.reshape(no_of_datapoints,col_dim)
x_data=x_data.T   #input is x_data

#trainform the test dataset 
no_of_datapoints_test=test_X.shape[0]
x_test=test_X.reshape(no_of_datapoints_test,col_dim)
x_test=x_test.T  #to test use this data

def layers_data():
  no_hidden_layers=1
  neuron_each_layer=[]
  #for loop to take entry and append
  neuron_each_layer.append(128)
  
  


  return neuron_each_layer

def init_layer(input,output):
  #create weight and bias matrix
  #np.random.normal(0,0.5,(layers[i],layers[i+1]))
  """ 
  
  """
  w=np.random.normal(0,0.5,(output,input)) #128*784
  b=np.random.normal(0,0.5,(output,1))
  return w,b

def sigmoid(z):
  #normalize data before use
  
  g = 1 / (1 + np.exp(-z))
  return g

def softmax(x):
  x=x.T
  for i in range(x.shape[0]):
    sum=0
    minidx=np.argmin(x[i])
    maxidx=np.argmax(x[i])
    x[i]=(x[i]-x[i][minidx])/(x[i][maxidx]-x[i][minidx])
    for j in range(x.shape[1]):
      sum=sum+np.exp(x[i][j])
    x[i]=np.exp(x[i])/sum
  x=x.T
  return x

def sigmoid_der(z):
  return np.multiply((1/(1+np.exp(-z))),1-((1/(1+np.exp(-z)))))

def forward_propogation(input,weight,bias,no_of_hidden_layers,output):
  activation=[]  #input to above layer
  a=[] 
  #no of layers =  no of layers
  #a=wx+b   and h is sigmoid of a   input data is 
  activation.append(input)
  a.append(input)
  for i in range(no_of_hidden_layers):
    preactivation = np.matmul(weight[i],activation[i])+bias[i]
    a.append(preactivation)
    activation.append(sigmoid(preactivation))
  
  preactivation=np.matmul(weight[-1],activation[-1])+bias[-1]
  a.append(preactivation)
  activation.append(softmax(preactivation))

  loss=0
  temp=activation[-1].T
  for i in range(input.shape[1]):
    loss+=-np.log2(temp[i][output[i]])
  return activation,loss,a

def backward_propogation(activation,output,no_hidden_layers,weights,a):
  #gradient_activation=[]
  gradient_weight=[]
  gradient_bias=[]
  
  # we need one hot vector for output in the format 10*60000 
  hot=np.zeros((activation[-1].shape[1],activation[-1].shape[0]))
  for i in range(activation[-1].shape[1]):
    hot[i][output[i]]=1
  
  hot=hot.T
  gradient_weight=[]
  gradiend_bias=[]
  gradient_activation=-(hot-activation[-1])  #gradient al

  for i in range(no_hidden_layers,-1,-1):
    weight_temp=np.matmul(gradient_activation,activation[i].T)
    bias_temp=(np.sum(gradient_activation,axis=1)/activation[i].shape[1]).reshape(-1,1)
    gradient_weight.append(weight_temp)
    gradient_bias.append(bias_temp)
    #print(gradient_bias[i].shape)
    if(i!=0):
      gradient_temp=np.matmul(weights[i].T,gradient_activation)
      gradient_activation=gradient_temp*sigmoid_der(a[i])
  gradient_weight.reverse()
  gradient_bias.reverse()
  return gradient_weight,gradient_bias

def gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate):
  for i in range(epochs):
    activation,loss,a=forward_propogation(input,weight,bias,no_of_hidden_layers,output)
    gradient_weight,gradient_bias=backward_propogation(activation,output,no_of_hidden_layers,weight,a) 
    for i in range(len(weight)):
      weight[i]=weight[i]-learning_rate*gradient_weight[i]
      bias[i]=bias[i]-learning_rate*gradient_bias[i]

    print(loss/input.shape[1])

def batch_gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size):
  for i in range(epochs):
    # take theinput and spilt it in batch and then provide that as input to
    for j in range(0,input.shape[1],batch_size):
      x_batch= input[:,j:j+batch_size]
      activation,loss,a=forward_propogation(x_batch,weight,bias,no_of_hidden_layers,output)
      gradient_weight,gradient_bias=backward_propogation(activation,output,no_of_hidden_layers,weight,a)
      for k in range(len(weight)):
        weight[k]=weight[k]-learning_rate*gradient_weight[k]
        bias[k]=bias[k]-learning_rate*gradient_bias[k]

    
    print("epoch is ", i ," " ,loss/batch_size)

neuron_each_layer=layers_data()
w=[]
b=[]
weight,bias=init_layer(784,128)
w.append(weight)
b.append(bias)
weight,bias=init_layer(128,10)
w.append(weight)
b.append(bias)

gradient_descent(x_data,w,b,len(neuron_each_layer),train_Y,20,0.00001)

batch_gradient_descent(x_data,w,b,len(neuron_each_layer),train_Y,20,0.1,32)

