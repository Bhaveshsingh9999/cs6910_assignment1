# -*- coding: utf-8 -*-
"""workingnadam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mhl0IAwIDn5mEeGcWXiaZo8AGgGZvsLZ
"""

import numpy as np
from keras.datasets import fashion_mnist
(train_X,train_Y),(test_X,test_Y)=fashion_mnist.load_data()
#normalize the train dataset as we values are from 0-255
train_X=train_X/255
test_X=test_X/255

#transforming dataset to 2d matrix whith col representing the data  
no_of_datapoints=train_X.shape[0]
col_dim=train_X.shape[1]*train_X.shape[2]
x_data=train_X.reshape(no_of_datapoints,col_dim)
x_data=x_data.T   #input is x_data

#trainform the test dataset 
no_of_datapoints_test=test_X.shape[0]
x_test=test_X.reshape(no_of_datapoints_test,col_dim)
x_test=x_test.T  #to test use this data

# def init_layer(no_neuron_input,no_neuron_output,weight,bias):
 
#   #weight.append(np.random.normal(0,0.5,(no_neuron_output,no_neuron_input))) #128*784
#   #bias.append(np.random.normal(0,0.5,(no_neuron_output,1)))
#   np.random.seed(10)
#   weight.append(np.random.uniform(-1,1,(no_neuron_output,no_neuron_input)))
#   bias.append(np.random.normal(-1,1,(no_neuron_output,1)))

def sigmoid(z):
  g = 1 / (1 + np.exp(-z))
  return g

def tanh(a):
  g=((np.exp(a) - np.exp(-a))/(np.exp(a) + np.exp(-a)))
  return g

def relu(z):
  return np.maximum(0,z)

def softmax(x):
  x=x.T
  for i in range(x.shape[0]):
    sum=0
    max_ele=np.max(x[i])
    for j in range(x.shape[1]):
      sum+=np.exp(x[i][j]-max_ele)
    x[i]=np.exp(x[i]-max_ele)/sum
  x=x.T

  return x

def sigmoid_der(z):
  t=sigmoid(z)
  return np.multiply(t,(1-t))

def tanh_der(a):
  t=((np.exp(a) - np.exp(-a))/(np.exp(a) + np.exp(-a)))
  return 1-np.square(t)

def relu_der(a):
  a[a<=0]=0
  a[a>0]=1
  return a

def cross_entropy_loss(y_hat,y):
  loss=0
  
  for i in range(y.shape[0]):
    loss+=-np.log2(y_hat[i][y[i]])

  loss=loss/y.shape[0]
  
  return loss

def mse_loss(y_hat,y):
  loss=0
  for i in range(y_hat.shape[0]):
    for j in range(y_hat.shape[1]):
      if(j!=y[i]):
        loss+=np.square(y_hat[i][j])
      else:
        loss+=np.square(y_hat[i][j]-1)
  loss=loss/y_hat.shape[0]

  

  return loss

def forward_propogation(input,weight,bias,no_of_hidden_layers,output,activation_function,loss_fn):
  activation=[]  #input to above layer
  preactivation=[] 
  #no of layers =  no of layers
  #a=wx+b   and h is sigmoid of a   input data is 
  activation.append(input)
  preactivation.append(input)
  for i in range(no_of_hidden_layers):
    #print(weight[i].shape,' ',activation[i].shape,' ',bias[i].shape)
    temp_preactive = np.matmul(weight[i],activation[i])+bias[i]
    
    if(i==0 and activation_function=='relu'):
      temp_preactive=temp_preactive.T
      for i in range(temp_preactive.shape[0]):
        temp_preactive[i]=temp_preactive[i]/np.max(temp_preactive[i])
      temp_preactive=temp_preactive.T


    preactivation.append(temp_preactive)
    #print(temp_preactive[:,0])
    if(activation_function=='sigmoid'):
      activation.append(sigmoid(np.copy(temp_preactive)))
    elif(activation_function=='tanh'):
      activation.append(tanh(np.copy(temp_preactive)))
    elif(activation_function=='relu'):
      activation.append(relu(np.copy(temp_preactive)))
    #print(activation[-1][:,0])

  
  temp_preactive=np.matmul(weight[-1],activation[-1])+bias[-1]
  

  preactivation.append(temp_preactive)
  #print(temp_preactive[:,0])
  activation.append(softmax(np.copy(temp_preactive)))

  #print(activation[-1][:,0])

  
  temp=activation[-1].T
  if(loss_fn=='cross_entropy'):
    loss=cross_entropy_loss(temp,output)
  elif(loss_fn=='mse'):
    loss=mse_loss(temp,output)
  
  
  return activation,loss,preactivation

def backward_propogation(activation,output,no_hidden_layers,weights,a,batch_size,activation_function):
  #gradient_activation=[]
  gradient_weight=[]
  gradient_bias=[]
  
  # we need one hot vector for output in the format 10*60000 
  hot=np.zeros((activation[-1].shape[1],activation[-1].shape[0]))
  for i in range(activation[-1].shape[1]):
    hot[i][output[i]]=1
  
  hot=hot.T
  gradient_weight=[]
  gradiend_bias=[]
  gradient_activation=-(hot-activation[-1])  #gradient al

  for i in range(no_hidden_layers,-1,-1):
    weight_temp=np.matmul(gradient_activation,activation[i].T)/batch_size
    bias_temp=(np.sum(gradient_activation,axis=1)/batch_size).reshape(-1,1)
    gradient_weight.append(weight_temp)
    gradient_bias.append(bias_temp)
    #print(gradient_bias[i].shape)
    if(i!=0):
      gradient_temp=np.matmul(weights[i].T,gradient_activation)
      #gradient_activation=gradient_temp*sigmoid_der(a[i]) thhis below is for sigmoid baki ka likhna hai relu tanh a ias preactivation
      if(activation_function=='sigmoid'):
        gradient_activation=gradient_temp*sigmoid_der(np.copy(a[i]))
      elif(activation_function=='tanh'):
        gradient_activation=gradient_temp*(tanh_der(np.copy(a[i])))
      elif(activation_function=='relu'):
        gradient_activation=gradient_temp*(np.copy(relu_der(a[i])))
  gradient_weight.reverse()
  gradient_bias.reverse()
  return gradient_weight,gradient_bias

def gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size,activation_function,loss_fn):
  for i in range(epochs):
    activation,loss,a=forward_propogation(input,weight,bias,no_of_hidden_layers,output,activation_function,loss_fn)
    gradient_weight,gradient_bias=backward_propogation(activation,output,no_of_hidden_layers,weight,a,batch_size,activation_function) 
    for i in range(len(weight)):
      weight[i]=weight[i]-learning_rate*gradient_weight[i]
      bias[i]=bias[i]-learning_rate*gradient_bias[i]

    print(loss/input.shape[1])

def batch_gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size,activation_function,loss_fn):
  no_batches=input.shape[1]/batch_size
  for i in range(epochs):
    # take theinput and spilt it in batch and then provide that as input to
    loss_total=0
    for j in range(0,input.shape[1],batch_size):
      x_batch= input[:,j:j+batch_size]
      y_batch=  output[j:j+batch_size] 
      activation,loss,a=forward_propogation(x_batch,weight,bias,no_of_hidden_layers,y_batch,activation_function,loss_fn)
      gradient_weight,gradient_bias=backward_propogation(activation,y_batch,no_of_hidden_layers,weight,a,batch_size,activation_function)

      loss_total+=loss
      
      for k in range(len(weight)):
        weight[k]=weight[k]-learning_rate*gradient_weight[k]
        bias[k]=bias[k]-learning_rate*gradient_bias[k]

    print("epoch is ", i , j+batch_size,loss_total/no_batches)
    test_data_accuracy(x_test,weight,bias,no_hidden_layer,test_Y,activation_function,loss_fn)

def momentum_gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size,beta,activation_function,loss_fn):
  ut_weight=[]
  ut_bias=[]
  ut_weight.extend([0 for i in range(len(weight))])
  ut_bias.extend([0 for i in range(len(bias))])
  
  no_batches=input.shape[1]/batch_size
  for i in range(epochs):
    # take theinput and spilt it in batch and then provide that as input to
    loss_total=0
    for j in range(0,input.shape[1],batch_size):
      x_batch= input[:,j:j+batch_size]
      y_batch=  output[j:j+batch_size] 
      activation,loss,a=forward_propogation(x_batch,weight,bias,no_of_hidden_layers,y_batch,activation_function,loss_fn)
      gradient_weight,gradient_bias=backward_propogation(activation,y_batch,no_of_hidden_layers,weight,a,batch_size,activation_function)
      #print(loss)

      loss_total+=loss

      

      for k in range(len(weight)):
        ut_weight[k]=beta*ut_weight[k]+learning_rate*gradient_weight[k]
        ut_bias[k]=beta*ut_bias[k]+learning_rate*gradient_bias[k]
        weight[k]=weight[k]-ut_weight[k]
        bias[k]=bias[k]-ut_bias[k]

    print("epoch is ", i ,loss_total/(no_batches))
    test_data_accuracy(x_test,weight,bias,no_hidden_layer,test_Y,activation_function,loss_fn)

def nesterov_gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size,beta,activation_function,loss_fn):
  ut_weight=[]
  ut_bias=[]
  ut_weight.extend([0 for i in range(len(weight))])
  ut_bias.extend([0 for i in range(len(bias))])
  no_batches=input.shape[1]/batch_size
  for i in range(epochs):
    # take theinput and spilt it in batch and then provide that as input to
    loss_total=0
    for j in range(0,input.shape[1],batch_size):
      x_batch= input[:,j:j+batch_size]
      y_batch=  output[j:j+batch_size]
      for k in range(len(weight)):
        weight[k]= weight[k]-beta*ut_weight[k]
        bias[k]=bias[k]-beta*ut_bias[k]
      activation,loss,a=forward_propogation(x_batch,weight,bias,no_of_hidden_layers,y_batch,activation_function,loss_fn)
      gradient_weight,gradient_bias=backward_propogation(activation,y_batch,no_of_hidden_layers,weight,a,batch_size,activation_function)
      #print(loss)

      loss_total+=loss

      

      for k in range(len(weight)):
        ut_weight[k]=beta*ut_weight[k]+learning_rate*gradient_weight[k]
        ut_bias[k]=beta*ut_bias[k]+learning_rate*gradient_bias[k]
        weight[k]=weight[k]-ut_weight[k]
        bias[k]=bias[k]-ut_bias[k]

    print("epoch is ", i ,loss_total/no_batches)
    test_data_accuracy(x_test,weight,bias,no_hidden_layer,test_Y,activation_function,loss_fn)

def rmsprop_gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size,beta,epsillon,activation_function,loss_fn):
  ut_weight=[]
  ut_bias=[]
  ut_weight.extend([0 for i in range(len(weight))])
  ut_bias.extend([0 for i in range(len(bias))])
  no_batches=input.shape[1]/batch_size
  for i in range(epochs):
    # take theinput and spilt it in batch and then provide that as input to
    loss_total=0
    for j in range(0,input.shape[1],batch_size):
      x_batch= input[:,j:j+batch_size]
      y_batch=  output[j:j+batch_size] 
      activation,loss,a=forward_propogation(x_batch,weight,bias,no_of_hidden_layers,y_batch,activation_function,loss_fn)
      gradient_weight,gradient_bias=backward_propogation(activation,y_batch,no_of_hidden_layers,weight,a,batch_size,activation_function)
      #print(loss)

      loss_total+=loss

      

      for k in range(len(weight)):
        ut_weight[k]=beta*ut_weight[k]+(1-beta)*np.square(gradient_weight[k])
        ut_bias[k]=beta*ut_bias[k]+(1-beta)*np.square(gradient_bias[k])
        weight[k]=weight[k]-learning_rate*(gradient_weight[k]/np.sqrt(ut_weight[k]+epsillon))
        bias[k]=bias[k]-learning_rate*(gradient_bias[k]/np.sqrt(ut_bias[k]+epsillon))

    print("epoch is ", i ,loss_total/no_batches)
    test_data_accuracy(x_test,weight,bias,no_hidden_layer,test_Y,activation_function,loss_fn)

def adam_gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size,beta1,beta2,epsillon,activation_function,loss_fn):
  mt_weight=[]
  mt_bias=[]
  vt_weight=[]
  vt_bias=[]
  mt_weight.extend([0 for i in range(len(weight))])
  mt_bias.extend([0 for i in range(len(bias))])
  vt_weight.extend([0 for i in range(len(weight))])
  vt_bias.extend([0 for i in range(len(bias))])

  mt_hat_weight=[]
  vt_hat_weight=[]
  mt_hat_bias=[]
  vt_hat_bias=[]
  mt_hat_weight.extend([0 for i in range(len(weight))])
  vt_hat_weight.extend([0 for i in range(len(weight))])
  mt_hat_bias.extend([0 for i in range(len(weight))])
  vt_hat_bias.extend([0 for i in range(len(weight))])
  no_batches=input.shape[1]/batch_size



  t=0
  for i in range(epochs):
    # take theinput and spilt it in batch and then provide that as input to
    loss_total=0
    for j in range(0,input.shape[1],batch_size):
      t+=1
      x_batch= input[:,j:j+batch_size]
      y_batch=  output[j:j+batch_size] 
      activation,loss,a=forward_propogation(x_batch,weight,bias,no_of_hidden_layers,y_batch,activation_function,loss_fn)
      gradient_weight,gradient_bias=backward_propogation(activation,y_batch,no_of_hidden_layers,weight,a,batch_size,activation_function)
      #print(loss)

      loss_total+=loss

      for k in range(len(weight)):
        mt_weight[k]=beta1*mt_weight[k]+(1-beta1)*gradient_weight[k]
        mt_bias[k]=beta1*mt_bias[k]+(1-beta1)*gradient_bias[k]
        
        mt_hat_weight[k]=mt_weight[k]/(1-(beta1**t))
        mt_hat_bias[k]=mt_bias[k]/(1-(beta1**t))



        vt_weight[k]=beta2*vt_weight[k]+(1-beta2)*(np.square(gradient_weight[k]))
        vt_bias[k]=beta2*vt_bias[k]+(1-beta2)*(np.square(gradient_weight[k]))
        
        vt_hat_weight[k]=vt_hat_weight[k]/(1-(beta2**t))
        vt_hat_bias[k]=vt_hat_bias[k]/(1-(beta2**t))

        weight[k]=weight[k]-(learning_rate*(np.divide(mt_hat_weight[k],(np.sqrt(vt_hat_weight[k])+epsillon))))
        bias[k]=bias[k]-(learning_rate)*(np.divide(mt_hat_bias[k],(np.sqrt(vt_hat_bias[k])+epsillon)))


    print("epoch is ", i ," j from " ,j,"to", j+batch_size,loss_total/no_batches)
    test_data_accuracy(x_test,weight,bias,no_hidden_layer,test_Y,activation_function,loss_fn)

def nadam_gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate,batch_size,beta1,beta2,epsillon,activation_function,loss_fn):
  mt_weight=[]
  mt_bias=[]
  vt_weight=[]
  vt_bias=[]
  mt_weight.extend([0 for i in range(len(weight))])
  mt_bias.extend([0 for i in range(len(bias))])
  vt_weight.extend([0 for i in range(len(weight))])
  vt_bias.extend([0 for i in range(len(bias))])

  mt_hat_weight=[]
  vt_hat_weight=[]
  mt_hat_bias=[]
  vt_hat_bias=[]
  mt_hat_weight.extend([0 for i in range(len(weight))])
  vt_hat_weight.extend([0 for i in range(len(weight))])
  mt_hat_bias.extend([0 for i in range(len(weight))])
  vt_hat_bias.extend([0 for i in range(len(weight))])
  no_batches=input.shape[1]/batch_size



  t=0
  for i in range(epochs):
    # take theinput and spilt it in batch and then provide that as input to
    loss_total=0
    for j in range(0,input.shape[1],batch_size):
      t+=1
      x_batch= input[:,j:j+batch_size]
      y_batch=  output[j:j+batch_size] 
      activation,loss,a=forward_propogation(x_batch,weight,bias,no_of_hidden_layers,y_batch,activation_function,loss_fn)
      gradient_weight,gradient_bias=backward_propogation(activation,y_batch,no_of_hidden_layers,weight,a,batch_size,activation_function)
      #print(loss)

      loss_total+=loss

      for k in range(len(weight)):
        mt_weight[k]=beta1*mt_weight[k]+(1-beta1)*gradient_weight[k]
        mt_bias[k]=beta1*mt_bias[k]+(1-beta1)*gradient_bias[k]
        
        mt_hat_weight[k]=mt_weight[k]/(1-(beta1**t))
        mt_hat_bias[k]=mt_bias[k]/(1-(beta1**t))



        vt_weight[k]=beta2*vt_weight[k]+(1-beta2)*(np.square(gradient_weight[k]))
        vt_bias[k]=beta2*vt_bias[k]+(1-beta2)*(np.square(gradient_weight[k]))
        
        vt_hat_weight[k]=vt_hat_weight[k]/(1-(beta2**t))
        vt_hat_bias[k]=vt_hat_bias[k]/(1-(beta2**t))

        weight[k]=weight[k]-(learning_rate*(np.divide(beta1*mt_hat_weight[k]+((1-beta1)*gradient_weight[k])/(1-(beta1**t)),(np.sqrt(vt_hat_weight[k])+epsillon))))
        bias[k]=bias[k]-(learning_rate)*(np.divide(beta1*mt_hat_bias[k]+((1-beta1)*gradient_bias[k])/(1-(beta1**t)),(np.sqrt(vt_hat_bias[k])+epsillon)))


    print("epoch is ", i ,loss_total/no_batches)
    test_data_accuracy(x_test,weight,bias,no_hidden_layer,test_Y,activation_function,loss_fn)





def init_layer(no_hidden_layer,hidden_layer_size,weight_init_fn='random'):
  weight=[]
  bias=[]
  #initialize h1 weight and bias
  layer=[]
  layer.append(784)
  for i in range(no_hidden_layer):
    layer.append(hidden_layer_size)
  layer.append(10)
  np.random.seed(10)

  if(weight_init_fn=='random'):
     for i in range(no_hidden_layer+1):
       weight.append(np.random.uniform(-1,1,(layer[i+1],layer[i])))
       bias.append(np.random.uniform(-1,1,(layer[i+1],1)))

  

  

  return weight,bias

no_hidden_layer=3
hidden_layer_size=128
weight,bias = init_layer(no_hidden_layer,hidden_layer_size,'random')

gradient_descent(x_data,weight,bias,len(weight)-1,train_Y,15,0.001,x_data.shape[1])

batch_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.1,32,'sigmoid','cross_entropy')

batch_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.01,32,'relu','cross_entropy')

batch_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.01,32,'relu','cross_entropy')

momentum_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.1,32,0.9,'relu','cross_entropy')

momentum_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.001,32,0.9,'relu','cross_entropy')

momentum_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.01,32,0.9,'tanh','cross_entropy')

momentum_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.1,32,0.9,'sigmoid')

nesterov_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.01,32,0.9,'tanh','cross_entropy')

rmsprop_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,20,0.01,32,0.9,0.01,'tanh','cross_entropy')

adam_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.001,32,0.9,0.99,0.01,'relu','cross_entropy')

nadam_gradient_descent(x_data,weight,bias,no_hidden_layer,train_Y,10,0.001,32,0.9,0.99,0.01,'relu','cross_entropy')

def test_data_accuracy(test_input,weight,bias,no_hidden_layers,test_output,activation_fn,loss_fn):
  activation=[]
  activation,loss,preactivation=forward_propogation(test_input,weight,bias,no_hidden_layers,test_output,activation_fn,loss_fn)
  y_hat=np.argmax(activation[-1].T,axis=1)
  count=0
  for i in range(test_output.shape[0]):
    if (y_hat[i]==test_output[i]):
      count+=1
  accuracy=count/test_output.shape[0]
  print("accuracy is ",accuracy)

test_data_accuracy(x_test,weight,bias,no_hidden_layer,test_Y,'sigmoid','cross_entropy')
