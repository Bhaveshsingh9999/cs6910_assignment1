# -*- coding: utf-8 -*-
"""Copy of question2firstattempt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hakgfLAioqaIUKHJN9x1IvZHXXrEH6kb
"""

import numpy as np
from keras.datasets import fashion_mnist
(train_X,train_Y),(test_X,test_Y)=fashion_mnist.load_data()
#normalize the train dataset as he valoes are from 0-255
train_X=train_X/255
test_X=test_X/255

#transforming dataset and then 
no_of_datapoints=train_X.shape[0]
col_dim=train_X.shape[1]*train_X.shape[2]
x_data=train_X.reshape(no_of_datapoints,col_dim)
x_data=x_data.T   #input is x_data

#trainform the test dataset 
no_of_datapoints_test=test_X.shape[0]
x_test=test_X.reshape(no_of_datapoints_test,col_dim)
x_test=x_test.T  #to test use this data

def layers_data():
  no_hidden_layers=1
  neuron_each_layer=[]
  #for loop to take entry and append
  neuron_each_layer.append(128)
  #neuron_each_layer.append(5)

  return neuron_each_layer

def weight_bias_init(neuron_each_layer,input_size=784,output_size=10):
  input_layer_neuron = input_size
  output_layer_neuron = output_size
    
  weights=[]
  bias=[]

  #for weight and bias between input and hidden 
  weights.append(np.random.random((neuron_each_layer[0],input_layer_neuron)))
  bias.append(np.random.rand(neuron_each_layer[0],1))

  #for weight and bias between 
  for i in range(len(neuron_each_layer)-1):
    weights.append(np.random.random((neuron_each_layer[i+1],neuron_each_layer[i])))
    bias.append(np.random.rand(neuron_each_layer[i+1],1))
  
  weights.append(np.random.random((output_layer_neuron,neuron_each_layer[-1])))
  bias.append(np.random.rand(output_layer_neuron,1))

  return weights,bias

#for check

def sigmoid(z):
  #normalize data before use
  z=z.T
  for i in range(z.shape[0]):
    max=np.argmax(z[i])
    z[i]=z[i]/z[i][max]
  z=z.T
  g = 1 / (1 + np.exp(-z))
  return g

def ReLU(Z):
    return np.maximum(Z, 0)

def softmax(z):
  z=z.T
  #first normalize
  for i in range(z.shape[0]):
    max=np.argmax(z[i])
    z[i]=z[i]/z[i][max]
    total=0
    for j in range(z.shape[1]):
      total=total+np.exp(z[i][j])
    z[i]=np.exp(z[i])/total
  z=z.T
  return z

def forward_propogation(input,weight,bias,no_of_hidden_layers,output):
  activation=[]  #input to above layer 
  
  #no of layers =  no of layers
  #a=wx+b   and h is sigmoid of a   input data is 
  activation.append(input)
  for i in range(no_of_hidden_layers):
    preactivation = np.matmul(weight[i],activation[i])+bias[i]
    activation.append(sigmoid(preactivation))
  
  preactivation=np.matmul(weight[-1],activation[-1])+bias[-1]
  activation.append(softmax(preactivation))

  loss=0
  temp=activation[-1].T
  for i in range(input.shape[1]):
    loss+=-np.log2(temp[i][output[i]])
    

  return activation,loss

'''neuron_each_layer=layers_data()  # is a list which contain the number of neuron in each hidden  layer 
weight,bias=weight_bias_init(neuron_each_layer)
activation,loss=forward_propogation(x_data,weight,bias,len(neuron_each_layer),train_Y)'''

''''loss=0
c=activation[-1].T
for i in range(x_data.shape[1]):
  loss+=-np.log2(c[i][train_Y[i]])'''

'''one_hot_matrix=np.zeros((activation[-1].shape[1],activation[-1].shape[0]))
for i in range(activation[-1].shape[1]):
  one_hot_matrix[i][train_Y[i]]=1

print(train_Y.shape)'''

a=np.ones((3,4))
print(1-a)

def backward_propogation(activation,output,no_hidden_layers,weights):
  gradient_activation=[]
  gradient_weight=[]
  gradient_bias=[]
  
  # just to set the size of the list
  gradient_activation.extend([0 for i in range(no_hidden_layers+1)])
  gradient_weight.extend([0 for i in range(no_hidden_layers+1)])
  gradient_bias.extend([0 for i in range(no_hidden_layers+1)])

  # we need one hot vector for output in the format 10*60000 
  one_hot_matrix=np.zeros((activation[-1].shape[1],activation[-1].shape[0]))
  for i in range(activation[-1].shape[1]):
    one_hot_matrix[i][output[i]]=1
  
  one_hot_matrix=one_hot_matrix.T
    
  
  gradient_activation[-1]=(-1*(one_hot_matrix-activation[-1]))  #gradient al
  

  for i in range(no_hidden_layers,-1,-1):
    gradient_weight[i]=np.matmul(gradient_activation[i],activation[i].T)
    gradient_bias[i]=(np.sum(gradient_activation[i],axis=1)/activation[-1].shape[1]).reshape(-1,1)
    #print(gradient_bias[i].shape)

    if(i!=0):
      gradient_temp=np.matmul(weights[i].T,gradient_activation[i])
      gradient_activation[i-1]=gradient_temp*activation[i]*(1-activation[i])
      
      

  

  return gradient_weight,gradient_bias

l=[] 
for i in train_Y:
  if i not in l:
    l.append(i)
print(l)

def sigmoid_derivative(z):
  return np.multiply(sigmoid(z),(1-sigmoid(z)))

c,d=backward_propogation(activation,train_Y,len(neuron_each_layer))
for i in d:
  print(i.shape)

def cross_entropy(yhat,y_train):
  #chalo game shuru karte hain
  #yhat n*10
  sum=0 
  for i in range(y_train.shape[0]):
    sum+=-((np.log2(yhat[i][y_train[i]])))
  return sum

def gradient_descent(input,weight,bias,no_of_hidden_layers,output,epochs,learning_rate):
  for i in range(epochs):
    activation,loss=forward_propogation(input,weight,bias,no_of_hidden_layers,output)
    gradient_weight,gradient_bias=backward_propogation(activation,output,no_of_hidden_layers,weight) 
    for i in range(len(weight)):
      weight[i]=weight[i]-learning_rate*gradient_weight[i]
      bias[i]=bias[i]-learning_rate*gradient_bias[i]

    print(loss/60000)

print(x_data.shape)

neuron_each_layer=layers_data()
weight,bias=weight_bias_init(neuron_each_layer)

gradient_descent(x_data,weight,bias,len(neuron_each_layer),train_Y,10,0.000001)

def test(x_test,w,b,no_hidden_l,y_test):
  activation=[]
  h=[]
  activation,loss=forward_propogation(x_test,w,b,no_hidden_l,y_test)
  l=len(w)
  ypred=np.argmax(activation[-1].T,axis=1)
  print(ypred)
  print(y_test)
  count=0
  for i in range(y_test.shape[0]):
    if ypred[i]!=y_test[i]:
      count=count+1
  print((x_test.shape[1]-count)/y_test.shape[0])

test(x_test,weight,bias,len(neuron_each_layer),test_Y)